{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all words as features\n",
      "train on 7998 instances, test on 2666 instances\n",
      "accuracy: 0.7753188297074268\n",
      "pos precision: 0.7747005988023952\n",
      "pos recall: 0.7764441110277569\n",
      "neg precision: 0.7759398496240602\n",
      "neg recall: 0.7741935483870968\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     17.0 : 1.0\n",
      "                   quiet = True              pos : neg    =     15.7 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "                portrait = True              pos : neg    =     12.4 : 1.0\n",
      "              refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "                   flaws = True              pos : neg    =     12.3 : 1.0\n",
      "                 triumph = True              pos : neg    =     11.7 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# http://andybromberg.com/sentiment-analysis-python/\n",
    "# Andy Bromberg's Simple Sentiment Analysis System\n",
    "# Uses data from Pang & Lee (2005)\n",
    "# Uses a Naive Bayes Classifier Train the System\n",
    "#  NB Updated 2016 for package changes around scores\n",
    "\n",
    "import re, math, collections, itertools, sys, os\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "from nltk.metrics import BigramAssocMeasures, scores\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from sklearn.svm import SVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(\"XLect10.Progs\")))\n",
    "\n",
    "\n",
    "def evaluate_features(feature_select):\n",
    "    #reading pre-labeled input and splitting into lines\n",
    "    negSentences = open(os.path.join(__location__, 'rt-polarity-neg.txt'), 'r', encoding='utf8')\n",
    "    posSentences = open(os.path.join(__location__, 'rt-polarity-pos.txt'), 'r', encoding='utf8')\n",
    "    #stopWords = open(os.path.join(__location__, 'stopwords.txt'), 'r', encoding='utf8')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    negSentences = re.split(r'\\n', negSentences.read())\n",
    "    posSentences = re.split(r'\\n', posSentences.read())\n",
    "    \n",
    "    \n",
    "    #Removal of stop words from Negative Sentences\n",
    "    for i in range(0,len(negSentences)):\n",
    "        word_tokens = word_tokenize(negSentences[i])\n",
    "        filtered_sentence = [] \n",
    "        for w in word_tokens: \n",
    "            if w not in stop_words: \n",
    "                filtered_sentence.append(w)\n",
    "        negSentences[i] = ' '.join(filtered_sentence)\n",
    "        \n",
    "    #Removal of stop words from Positive Sentences\n",
    "    for i in range(0,len(posSentences)):\n",
    "        word_tokens = word_tokenize(posSentences[i])\n",
    "        filtered_sentence = [] \n",
    "        for w in word_tokens: \n",
    "            if w not in stop_words: \n",
    "                filtered_sentence.append(w)\n",
    "        posSentences[i] = ' '.join(filtered_sentence)\n",
    "    \n",
    "    posFeatures = []\n",
    "    negFeatures = []\n",
    "    # breaks up the sentences into lists of individual words\n",
    "    # creates instance structures for classifier\n",
    "    for i in posSentences:\n",
    "        posWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        posWords = [feature_select(posWords), 'pos']\n",
    "        posFeatures.append(posWords)\n",
    "    for i in negSentences:\n",
    "        negWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        negWords = [feature_select(negWords), 'neg']\n",
    "        negFeatures.append(negWords)\n",
    "    posCutoff = int(math.floor(len(posFeatures)*3/4))\n",
    "    negCutoff = int(math.floor(len(negFeatures)*3/4))\n",
    "    \n",
    "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
    "    \n",
    "    #Runs the classifier on the testFeatures\n",
    "    #classifier = DecisionTreeClassifier.train(trainFeatures)\n",
    "    classifier = NaiveBayesClassifier.train(trainFeatures)\n",
    "    #classifier = SklearnClassifier(SVC(), sparse=False).train(trainFeatures)\n",
    "    \n",
    "    #Sets up labels to look at output\n",
    "    referenceSets = collections.defaultdict(set)\n",
    "    testSets = collections.defaultdict(set)\n",
    "    for i, (features, label) in enumerate(testFeatures): # enumerate adds number-count to each item\n",
    "        referenceSets[label].add(i)               # recorded polarity for these test sentences\n",
    "        predicted = classifier.classify(features) # classifiers' proposed polarity for tests\n",
    "        testSets[predicted].add(i)\n",
    "\n",
    "    #Outputs\n",
    "    print('train on %s instances, test on %s instances'% (len(trainFeatures), len(testFeatures)))\n",
    "    print('accuracy:', nltk.classify.util.accuracy(classifier, testFeatures))\n",
    "    print('pos precision:', scores.precision(referenceSets['pos'], testSets['pos']))\n",
    "    print('pos recall:', scores.recall(referenceSets['pos'], testSets['pos']))\n",
    "    print('neg precision:', scores.precision(referenceSets['neg'], testSets['neg']))\n",
    "    print('neg recall:', scores.recall(referenceSets['neg'], testSets['neg']))\n",
    "    classifier.show_most_informative_features(10)\n",
    "\n",
    "def make_full_dict(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "print('using all words as features')\n",
    "evaluate_features(make_full_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "from nltk.collocations import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_scores():\n",
    "    __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(\"XLect10.Progs\")))\n",
    "    #splits sentences into lines\n",
    "    posSentences = open(os.path.join(__location__, 'rt-polarity-pos.txt'), 'r')\n",
    "    negSentences = open(os.path.join(__location__, 'rt-polarity-neg.txt'), 'r')\n",
    "    \n",
    "    posSentences = re.split(r'\\n', posSentences.read())\n",
    "    negSentences = re.split(r'\\n', negSentences.read())\n",
    " \n",
    "    #creates lists of all positive and negative words\n",
    "    posWords = []\n",
    "    negWords = []\n",
    "    for i in posSentences:\n",
    "        posWord = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        posWords.append(posWord)\n",
    "    for i in negSentences:\n",
    "        negWord = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        negWords.append(negWord)\n",
    "    posWords = list(itertools.chain(*posWords))\n",
    "    negWords = list(itertools.chain(*negWords))\n",
    "    return (posWords,negWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_words(word_scores, number):\n",
    "    best_vals = sorted(word_scores.items(), key= lambda x: x[1], reverse=True)[:number]\n",
    "    best_words = set([w for w, s in best_vals])\n",
    "    return best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_word_features(words):\n",
    "    return dict([(word, True) for word in words if word in best_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fd = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_word_fd = ConditionalFreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList = create_word_scores()\n",
    "posWords = wordList[0]\n",
    "negWords = wordList[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in posWords:\n",
    "    word_fd[word.lower()] += 1\n",
    "    cond_word_fd['pos'][word.lower()]+= 1\n",
    "for word in negWords:\n",
    "    word_fd[word.lower()] += 1\n",
    "    cond_word_fd['neg'][word.lower()]+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_word_count = cond_word_fd['pos'].N()\n",
    "neg_word_count = cond_word_fd['neg'].N()\n",
    "total_word_count = pos_word_count + neg_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores = {}\n",
    "for word, freq in word_fd.items():\n",
    "    pos_score = BigramAssocMeasures.chi_sq(cond_word_fd['pos'][word], (freq, pos_word_count), total_word_count)\n",
    "    neg_score = BigramAssocMeasures.chi_sq(cond_word_fd['neg'][word], (freq, neg_word_count), total_word_count)\n",
    "    word_scores[word] = pos_score + neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating best 100 word features\n",
      "train on 7998 instances, test on 2666 instances\n",
      "accuracy: 0.6376594148537135\n",
      "pos precision: 0.5981808453718566\n",
      "pos recall: 0.8387096774193549\n",
      "neg precision: 0.7302383939774153\n",
      "neg recall: 0.436609152288072\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     17.0 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "                portrait = True              pos : neg    =     12.4 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "                    flat = True              neg : pos    =     11.4 : 1.0\n",
      "                  boring = True              neg : pos    =     11.3 : 1.0\n",
      "               beautiful = True              pos : neg    =     10.7 : 1.0\n",
      "                    warm = True              pos : neg    =     10.6 : 1.0\n",
      "                  stupid = True              neg : pos    =     10.6 : 1.0\n",
      "                touching = True              pos : neg    =     10.4 : 1.0\n",
      "evaluating best 15000 word features\n",
      "train on 7998 instances, test on 2666 instances\n",
      "accuracy: 0.8447111777944486\n",
      "pos precision: 0.841635687732342\n",
      "pos recall: 0.8492123030757689\n",
      "neg precision: 0.8478425435276306\n",
      "neg recall: 0.8402100525131283\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     17.0 : 1.0\n",
      "                   quiet = True              pos : neg    =     15.7 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "                portrait = True              pos : neg    =     12.4 : 1.0\n",
      "              refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "                   flaws = True              pos : neg    =     12.3 : 1.0\n",
      "                 triumph = True              pos : neg    =     11.7 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n",
      "evaluating best 30000 word features\n",
      "train on 7998 instances, test on 2666 instances\n",
      "accuracy: 0.7726931732933233\n",
      "pos precision: 0.769059955588453\n",
      "pos recall: 0.7794448612153039\n",
      "neg precision: 0.776425855513308\n",
      "neg recall: 0.7659414853713429\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     17.0 : 1.0\n",
      "                   quiet = True              pos : neg    =     15.7 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "                portrait = True              pos : neg    =     12.4 : 1.0\n",
      "              refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "                   flaws = True              pos : neg    =     12.3 : 1.0\n",
      "                 triumph = True              pos : neg    =     11.7 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "numbers_to_test = [100,15000,30000]\n",
    "#tries the best_word_features mechanism with each of the numbers_to_test of features\n",
    "for num in numbers_to_test:\n",
    "    print('evaluating best',num,'word features')\n",
    "    best_words = find_best_words(word_scores, num)\n",
    "    evaluate_features(best_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
